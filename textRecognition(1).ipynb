{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "textRecognition.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "XV-RyFVszaHF"
      },
      "source": [
        "import os\n",
        "import sys\n",
        "import csv\n",
        "import pandas as pd\n",
        "import matplotlib as plt\n",
        "%matplotlib inline\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import seaborn as sns\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P3_m3FDvJg9z"
      },
      "source": [
        "from keras.layers import Reshape, Embedding, Conv1D, MaxPooling1D,GlobalMaxPooling1D, Flatten, Dense,LSTM, GRU,Bidirectional,Dropout, Conv3D\n",
        "from tensorflow.keras import Input\n",
        "from keras.initializers import Constant\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow import keras\n",
        "import keras.backend as K\n",
        "from keras.layers import BatchNormalization\n",
        "import tensorflow as tf\n",
        "from keras.optimizers import RMSprop\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tqdm import tqdm\n",
        "\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e42TpZTEnfBD"
      },
      "source": [
        "from gensim.models import Word2Vec, KeyedVectors\n",
        "from collections import defaultdict"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BICyTK4pgSG2"
      },
      "source": [
        "## for bag-of-words\n",
        "from sklearn import feature_extraction, model_selection, manifold, preprocessing, feature_selection\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.pipeline import Pipeline"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bYqXj3VDc19b"
      },
      "source": [
        " ## Data Acquisition\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FffnMaPHyNK2"
      },
      "source": [
        "def downloadDataset():\n",
        "  if  not os.path.isfile('./drive/MyDrive/QS-OCR-small.tar.gz'):\n",
        "    print(\"Downloading... \")\n",
        "    ! wget https://github.com/QuickSign/ocrized-text-dataset/releases/download/v1.0/QS-OCR-small.tar.gz -P ./drive/MyDrive/\n",
        "    !mkdir ./drive/MyDrive/dataset\n",
        "    !tar -xf QS-OCR-small.tar.gz -C ./drive/MyDrive/dataset\n",
        "  else: \n",
        "    print(\"Dataset has already been downloaded. \")"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mLrznn2iyjjP"
      },
      "source": [
        "def createSingleDataset(filename):\n",
        "  # join all the file content into a single csv file\n",
        "  if not os.path.isfile(filename):\n",
        "    print(\"joining..\")\n",
        "    path = \"./drive/MyDrive/dataset\"\n",
        "    file_dataset = open(filename, mode='w')\n",
        "    file_writer = csv.writer(file_dataset, delimiter='|', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
        "    #walk trough the directory to get all the files' content\n",
        "    for dirpath, dirs, filenames in os.walk(path):\n",
        "      for filename in filenames:\n",
        "        f = open(dirpath +\"/\"+filename, \"r\")\n",
        "        getfilebody = f.read()\n",
        "        file_writer.writerow([filename, getfilebody,  dirpath.replace(\"./drive/MyDrive/dataset/\", \"\") ] )\n",
        "        f.close()\n",
        "  else:\n",
        "      print(\"Dataset files have already been joined. \")"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7CKmtd0IhsBo"
      },
      "source": [
        "def get_data(): \n",
        "  downloadDataset()\n",
        "  createSingleDataset('./drive/MyDrive/dataset.csv')\n",
        "  df = pd.read_csv(\"./drive/MyDrive/dataset.csv\", sep='|', delimiter=None, names = [\"filename\",\"content\", \"label\"] )\n",
        "  return df"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Eais14dmU2V3",
        "outputId": "05e83ef2-c936-4bad-eb15-184072b27c5e"
      },
      "source": [
        "import tensorflow as tf\n",
        "tf.test.gpu_device_name()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "''"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NLe6SfojTyE7"
      },
      "source": [
        "##Pre-Processing\n",
        "Data cleaning:\n",
        "  >* filling in missing values,\n",
        "  >*  smoothing noisy data,\n",
        "  >* identifying or removing outliers,\n",
        "  >* resolving inconsistencies,\n",
        "  >* duplicate samples\n",
        "  >* NLP pipeline = data normalizzation for text\n",
        "  >>* Tokenization\n",
        "  >>* punctuation removal\n",
        "  >>* stop words removal\n",
        "  >>* noise removal: Remove HTML tags, remove extra whitespaces, \n",
        "  >>* normalization: typo correction,  converting all characters to lowercase, number, date, symbol textualization, Expand contractions, remove_accented_chars\n",
        "  >>* lemmatization o steaming \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZnUOj6g-2pk3"
      },
      "source": [
        "def remove_tags(string):\n",
        "    result = re.sub('<.*?>',' ',string)\n",
        "    result = re.sub(r'\\b\\w{1,3}\\b', '', result) #remove words with legth < 3 chars\n",
        "    return result\n",
        "\n",
        "def clean_text(text):\n",
        "  for t in nlp.tokenizer(text):\n",
        "    if ((not t.is_stop) and ( not t.is_punct) and (not t.is_currency) and (not t.is_digit) and (not t.is_space) and (t.is_alpha) ):\n",
        "      return t.lemma_.lower().join(t.pos_)\n",
        "\n",
        "def execute_nlp(df):\n",
        "  nlp = spacy.load('en')  \n",
        "  #speech tag removed because it's too slow\n",
        "  df.insert(1, 'cleaned_text', df['content_without_tags'].apply(lambda x: [t.lemma_.lower() for t in nlp.tokenizer(x) if ((not t.is_stop) and ( not t.is_punct) and (not t.is_currency) and (not t.is_digit) and (not t.is_space) and (t.is_alpha) ) ]))\n",
        "  return df\n",
        "\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eWzqb2VImF6v",
        "outputId": "6c86372d-fe2a-4f6c-badf-9ee7fad3f82e"
      },
      "source": [
        "\n",
        "!pip install langid\n",
        "import spacy \n",
        "import en_core_web_sm\n",
        "import langid\n",
        "import re as re\n",
        "\n",
        "def pre_processing(df):\n",
        "  # nan removing\n",
        "  df = df.dropna()\n",
        "\n",
        "  #label encoding\n",
        "  categories = pd.unique(df[\"label\"])\n",
        "  labelencoder = LabelEncoder()\n",
        "  df['y'] = labelencoder.fit_transform(df['label'])\n",
        "  \n",
        "  #nlp pipeline\n",
        "\n",
        "  df['content']=df['content'].apply(str)\n",
        "  df.insert(1, 'content_without_tags', df[\"content\"].apply(lambda cw : remove_tags(cw)))  #removing HTML tags\n",
        "  df = execute_nlp(df)        \n",
        "\n",
        "  #remove unuseful column\n",
        "  df = df.drop(\"content_without_tags\", axis = 1)\n",
        "  df = df.drop(\"filename\", axis = 1)\n",
        "  \n",
        "  return df , categories\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: langid in /usr/local/lib/python3.6/dist-packages (1.1.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from langid) (1.19.5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J0DSnclkebFI"
      },
      "source": [
        "## Visualization and Dataset Analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AjpPy5e7N0zY",
        "outputId": "51050561-731a-4b60-db16-90425343e6c8"
      },
      "source": [
        "!pip install wordcloud\n",
        "!pip install pillow\n",
        "from wordcloud import WordCloud, ImageColorGenerator\n",
        "from PIL import Image\n",
        "\n",
        "def word_cloud(df):\n",
        "   \n",
        "  comment_words = '' \n",
        "  \n",
        "  # iterate through the csv file \n",
        "  for val in df.cleaned_text: \n",
        "        \n",
        "      # typecaste each val to string \n",
        "      val = str(val) \n",
        "    \n",
        "      # split the value \n",
        "      tokens = val.split() \n",
        "        \n",
        "      # Converts each token into lowercase \n",
        "      for i in range(len(tokens)): \n",
        "          tokens[i] = tokens[i].lower() \n",
        "        \n",
        "      comment_words += \" \".join(tokens)+\" \"\n",
        "    \n",
        "  wordcloud = WordCloud(width = 800, height = 800, \n",
        "                  background_color ='white', \n",
        "                  min_font_size = 10).generate(comment_words) \n",
        "      \n",
        "    # plot the WordCloud image                        \n",
        "  plt.pyplot.figure(figsize = (8, 8), facecolor = None) \n",
        "  plt.pyplot.imshow(wordcloud) \n",
        "  plt.pyplot.axis(\"off\") \n",
        "  plt.pyplot.title( pd.unique(df.label))\n",
        "\n",
        "  plt.pyplot.tight_layout(pad = 0) \n",
        "    \n",
        "  plt.pyplot.show() \n",
        "\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: wordcloud in /usr/local/lib/python3.6/dist-packages (1.5.0)\n",
            "Requirement already satisfied: numpy>=1.6.1 in /usr/local/lib/python3.6/dist-packages (from wordcloud) (1.19.5)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.6/dist-packages (from wordcloud) (7.0.0)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.6/dist-packages (7.0.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BNoYieWnewnW"
      },
      "source": [
        "### Utils"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1dFFEaZ12jsm"
      },
      "source": [
        "## split dataset\n",
        "def split_dataset(X, y):\n",
        "  #X = df.iloc[:, 0 ]\n",
        "  # y = df.iloc[:, 4 ]\n",
        "  X_train, X_test, y_train, y_test= model_selection.train_test_split(X, y, test_size=0.3, random_state = 42)\n",
        "  X_train, X_val, y_train, y_val = model_selection.train_test_split(X_train, y_train, test_size=0.2, random_state = 42)\n",
        "\n",
        "  #y_test = y_test.values.reshape((1045,1))\n",
        "  return X_train, X_test, y_train, y_test, X_val, y_val\n",
        "\n"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DS050Zu9nhjC"
      },
      "source": [
        "def get_most_frequent_word(number_of_word_to_get):\n",
        "  sentences = [row.split() for row in df['cleaned_text']] #tokenize sentence\n",
        "\n",
        "  #create dictionary word_freq(word, number_of_occurence)\n",
        "  word_freq = defaultdict(int)\n",
        "  for sent in sentences:\n",
        "      for i in sent:\n",
        "        # print(sent)\n",
        "          #print(i)\n",
        "          word_freq[i] += 1\n",
        "          \n",
        "  print(\"top 10 most frequent words:\", sorted(word_freq, key=word_freq.get, reverse=True)[:number_of_word_to_get])\n",
        "  #word_freq\n",
        "\n"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JJJptX2xBr1g"
      },
      "source": [
        "def get_f1(y_true, y_pred): #taken from old keras source code\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "    precision = true_positives / (predicted_positives + K.epsilon())\n",
        "    recall = true_positives / (possible_positives + K.epsilon())\n",
        "    f1_val = 2*(precision*recall)/(precision+recall+K.epsilon())\n",
        "    \n",
        "    return f1_val"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XxVRa1z2T7nk"
      },
      "source": [
        "def getPerformanceMetricsBERT(model, X_test, y_test, modelName):\n",
        "  print(modelName, \"metrics: \")\n",
        "  from sklearn.metrics import classification_report, accuracy_score,confusion_matrix,roc_curve,auc,precision_recall_curve\n",
        "  y_pred = model.predict(X_test)\n",
        "  #predicted_prob = model.predict_proba(X_test)\n",
        "  print('accuracy %s' % accuracy_score(y_pred.round(), y_test))\n",
        "  print(classification_report(y_test, y_pred.round(), target_names=categories))\n",
        "   ## Plot confusion matrix\n",
        "  #print(\"confusion Matrix\", modelName)\n",
        "  cm = confusion_matrix(y_test.argmax(axis=1), y_pred.argmax(axis=1))\n",
        "  fig, ax = plt.pyplot.subplots()\n",
        "  sns.heatmap(cm, annot=True, fmt='d', ax=ax, cmap=plt.cm.Blues, \n",
        "              cbar=False)\n",
        "  ax.set(xlabel=\"Pred\", ylabel=\"True\", xticklabels=categories, \n",
        "        yticklabels=categories, title=\"Confusion matrix \"+modelName)\n",
        "  plt.pyplot.yticks(rotation=0)\n",
        "  plt.pyplot.show()"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wxzauzGMFRJk"
      },
      "source": [
        "def plot_accuracy_loss(history): \n",
        "  plt.pyplot.subplot(211)\n",
        "  plt.pyplot.plot(history.history['get_f1'])\n",
        "  plt.pyplot.title('model f1_score')\n",
        "  plt.pyplot.ylabel('f1_score')\n",
        "  plt.pyplot.plot(history.history['val_get_f1'])\n",
        "  plt.pyplot.xlabel('epoch')\n",
        "  plt.pyplot.legend(['train', 'val'], loc='upper left')\n",
        "  plt.pyplot.subplot(212)\n",
        "  plt.pyplot.plot(history.history['loss'])\n",
        "  plt.pyplot.title('val loss')\n",
        "  plt.pyplot.ylabel('loss')\n",
        "  plt.pyplot.plot(history.history['val_loss'])\n",
        "  plt.pyplot.xlabel('epoch')\n",
        "  plt.pyplot.legend(['train', 'val'], loc='upper left')\n",
        "  plt.pyplot.show()"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9NiMCspHrMGG"
      },
      "source": [
        "## Baseline Model "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kSJKX-4ns_AS"
      },
      "source": [
        "\n",
        "def multinomialNBmodel():\n",
        "  nb = Pipeline([('vectorizer', feature_extraction.text.CountVectorizer()),\n",
        "                ('tfidf', feature_extraction.text.TfidfTransformer()),\n",
        "               ('clf', MultinomialNB()),])\n",
        "  return nb\n",
        "def SVMmodel():\n",
        "  svm = Pipeline([('vectorizer', feature_extraction.text.CountVectorizer()),\n",
        "                ('tfidf', feature_extraction.text.TfidfTransformer()),\n",
        "               ('clf',  SGDClassifier(penalty='l2', alpha=1e-3, random_state=42)),])\n",
        "  return svm\n",
        "\n",
        "def getPerformanceMetrics(model, X_test, y_test):\n",
        "  from sklearn.metrics import classification_report, accuracy_score,confusion_matrix,roc_curve,auc,precision_recall_curve\n",
        "  y_pred = model.predict(X_test)\n",
        "  #predicted_prob = model.predict_proba(X_test)\n",
        "  print('accuracy %s' % accuracy_score(y_pred, y_test))\n",
        "  print(classification_report(y_test, y_pred, target_names=categories))\n",
        "   ## Plot confusion matrix\n",
        "  cm = confusion_matrix(y_test, y_pred)\n",
        "  fig, ax = plt.pyplot.subplots()\n",
        "  sns.heatmap(cm, annot=True, fmt='d', ax=ax, cmap=plt.cm.Blues, \n",
        "              cbar=False)\n",
        "  ax.set(xlabel=\"Pred\", ylabel=\"True\", xticklabels=categories, \n",
        "        yticklabels=categories, title=\"Confusion matrix\")\n",
        "  plt.pyplot.yticks(rotation=0)\n",
        "#  fig, ax = plt.pyplot.subplots(nrows=1, ncols=2)\n",
        "  \n",
        "\n",
        "#NOTE: with MultinomialNB and tfidf there are label that are not predicted "
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jIfC53Gly61p"
      },
      "source": [
        "def compute_baseline(df):#model = multinomialNBmodel()\n",
        "  X_train, X_test, y_train, y_test, X_val, y_val = split_dataset(df.iloc[:, 0 ],df.iloc[:, 4])  #cleaned text and string label\n",
        "  model = SVMmodel()\n",
        "  model.fit(X_train, y_train)\n",
        "  getPerformanceMetrics(model, X_test, y_test)\n"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2O3-GiTuQI2t"
      },
      "source": [
        "## Modeling - Word Embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7vTY1E_pUXUb"
      },
      "source": [
        "####Create Customized Word Embedding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "65UFLCIvUpcg"
      },
      "source": [
        "def create_custom_word_embedding(df):\n",
        "    sentences = [row.split() for row in df['cleaned_text']] #tokenize sentence\n",
        "    w2v_model = Word2Vec(min_count=100, #min_count: minimum amount of time a word appear in word2vec training corpus  \n",
        "                        window=5,  # context window +- center word\n",
        "                        size=100,  # dimensions of word embeddings, also refer to size of hidden layer\n",
        "                        workers=4,\n",
        "                        sg =1 )#1= skip-gram, 0 = cbow\n",
        "                        \n",
        "    w2v_model.build_vocab(sentences)\n",
        "    w2v_model.train(sentences, total_examples=w2v_model.corpus_count, epochs=w2v_model.iter)\n",
        "    w2v_model.init_sims(replace=True)\n",
        "    print(\"vocab length\",len(w2v_model.wv.vocab))\n",
        "    return  w2v_model\n"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OXNqZBBbsdNv"
      },
      "source": [
        "#testing word embedding\n",
        "#Find the most similar words for \"smoke\"\n",
        "def test_word_embedding(w2v_model, word):\n",
        "  print(w2v_model.wv.most_similar(positive=[word]))\n",
        "\n"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nPGp2i8Ms9Is"
      },
      "source": [
        "from sklearn.manifold import TSNE\n",
        "def visualize_word2vec_model(model):\n",
        "    \"Create TSNE model and plot it\"\n",
        "    labels = []\n",
        "    tokens = []\n",
        "\n",
        "    for word in model.wv.vocab:\n",
        "        tokens.append(model[word])\n",
        "        labels.append(word)\n",
        "    \n",
        "    tsne_model = TSNE(perplexity=40, n_components=2, init='pca', n_iter=2500, random_state=23)\n",
        "    new_values = tsne_model.fit_transform(tokens)\n",
        "\n",
        "    x = []\n",
        "    y = []\n",
        "    for value in new_values:\n",
        "        x.append(value[0])\n",
        "        y.append(value[1])\n",
        "        \n",
        "    plt.pyplot.figure(figsize=(18, 18)) \n",
        "    for i in range(len(x)):\n",
        "        plt.pyplot.scatter(x[i],y[i])\n",
        "        plt.pyplot.annotate(labels[i],\n",
        "                     xy=(x[i], y[i]),\n",
        "                     xytext=(5, 2),\n",
        "                     textcoords='offset points',\n",
        "                     ha='right',\n",
        "                     va='bottom')\n",
        "    plt.pyplot.show()\n"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Q0m042s4PLO"
      },
      "source": [
        "def save_pretrained_word_embedding(filename, isBin, w2v_model):\n",
        "  w2v_model.wv.save_word2vec_format(filename, binary = isBin)\n"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IoKVx_Ek_tSH"
      },
      "source": [
        "#### Modeling Using Custom Word Embedding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q0-Ar6uK2b9E"
      },
      "source": [
        "def inits_params_word_embedding(vocab_len, max_sequence_length, val_split, embedding_dim ):\n",
        "  MAX_NB_WORDS = vocab_len\n",
        "  MAX_SEQUENCE_LENGTH = max_sequence_length # 800\n",
        "  VALIDATION_SPLIT = val_split  #0.2\n",
        "  EMBEDDING_DIM = embedding_dim #300 #has to be between 100 and 300\n",
        "  \n",
        "  return MAX_NB_WORDS, MAX_SEQUENCE_LENGTH, VALIDATION_SPLIT, EMBEDDING_DIM\n"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IBZSK8b3kyoc"
      },
      "source": [
        "def data_preparation(df,MAX_NB_WORDS,  MAX_SEQUENCE_LENGTH, VALIDATION_SPLIT, EMBEDDING_DIM):\n",
        "  #convert data into vector to fit word embedding:\n",
        "    # it means vectorize a text corpus, by turning each text into either a sequence of integers \n",
        "  tokenizer = Tokenizer(num_words = MAX_NB_WORDS)\n",
        "  tokenizer.fit_on_texts(df[\"cleaned_text\"])   #In the case where texts contains lists, we assume each entry of the lists to be a token.\n",
        "  sequences = tokenizer.texts_to_sequences(df[\"cleaned_text\"])  #turn each list into a \"sequence\": is a list of integer word indices. \n",
        "  word_index = tokenizer.word_index\n",
        "  print('Found %s unique tokens.' % len(word_index))\n",
        "\n",
        "  data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH) #X\n",
        "\n",
        "  labels = to_categorical(np.asarray(df[\"y\"])) #y\n",
        "  print('Shape of data tensor:', data.shape)\n",
        "  print('Shape of label tensor:', labels.shape)\n",
        "\n",
        "  # split the data into a training set and a validation set\n",
        "  \n",
        "  X_train, X_test, y_train, y_test, x_val, y_val = split_dataset(data, labels)\n",
        "    \n",
        "  return word_index, X_train, X_test, y_train, y_test, x_val, y_val, y_test\n"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wC1bfjI_27RD"
      },
      "source": [
        "def load_word_embedding(filename):\n",
        "  #extracting vocabs from 'word_embedding.txt'\n",
        "  embeddings_index = {}\n",
        "  f = open(os.path.join(\"\", filename),encoding='utf-8' )\n",
        "  for line in tqdm(f):\n",
        "      values = line.rstrip().rsplit(' ')\n",
        "      word = values[0]\n",
        "      coefs = np.asarray(values[1:], dtype='float32')\n",
        "      embeddings_index[word] = coefs\n",
        "  f.close()\n",
        "  print('Found %s word vectors.' % len(embeddings_index))\n",
        "\n",
        "  return embeddings_index"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xLBweDxO4Cww"
      },
      "source": [
        "#create embedding matrix that join pretrained word embedding with the dataset's vocabulary\n",
        "def create_embedding_matrix(embeddings_index, word_index, EMBEDDING_DIM):\n",
        "  embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
        "  for word, i in word_index.items():\n",
        "      embedding_vector = embeddings_index.get(word)\n",
        "      if embedding_vector is not None:\n",
        "          # words not found in embedding index will be all-zeros.\n",
        "          embedding_matrix[i] = embedding_vector\n",
        "  return embedding_matrix\n"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3bTSy6ptBGPI"
      },
      "source": [
        "def inits_params_embedding_layer(embedding_matrix, word_index, MAX_SEQUENCE_LENGTH, EMBEDDING_DIM):\n",
        "  print(\"embedding_layer\")\n",
        "  embedding_layer = Embedding(len(word_index) + 1,\n",
        "                            EMBEDDING_DIM,\n",
        "                            weights=[embedding_matrix],\n",
        "                            input_length=MAX_SEQUENCE_LENGTH,\n",
        "                            trainable=False)\n",
        "  return embedding_layer"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ik0vFj6d4eqT"
      },
      "source": [
        "def run_model(model,  x_train, y_train,  x_val, y_val, modelName):\n",
        "  model.compile(loss='categorical_crossentropy',\n",
        "                optimizer='rmsprop',\n",
        "                metrics=[get_f1, \"accuracy\"])\n",
        "  es_callback = EarlyStopping(monitor='val_loss', patience=3)\n",
        "  #history = model.fit(x_train, y_train, batch_size=128, epochs=30, validation_data=(x_val, y_val), callbacks=[es_callback], shuffle=False)\n",
        "  model.summary()\n",
        "  # happy learning!\n",
        "  history = model.fit(x_train, y_train, validation_data=(x_val, y_val),\n",
        "            epochs=20, batch_size=64,  callbacks=[es_callback], shuffle=False)\n",
        "  model.save(modelName)\n",
        "  \n",
        "  plot_accuracy_loss(history)\n",
        "\n",
        "  print(\"history\", history)\n",
        "  hist_df = pd.DataFrame(history.history) \n",
        "\n",
        "  hist_csv_file = './drive/MyDrive/history/history' + modelName.replace(\"./drive/MyDrive/\", \"\") + '.csv'   \n",
        "  with open(hist_csv_file, mode='w') as f:\n",
        "    hist_df.to_csv(f)\n",
        "  return model"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "YPk5ktRoj_qh",
        "outputId": "c6163db1-6fb4-43d1-f2d9-e11a2b575adb"
      },
      "source": [
        "\"\"\" #cose aggiunte\n",
        "  hist_df = pd.DataFrame(history.history) \n",
        "\n",
        "  hist_csv_file = 'history' + modelName.replace(\"./drive/MyDrive/\", \"\") + '.csv'   \n",
        "  with open(hist_csv_file, mode='w') as f:\n",
        "    hist_df.to_csv(f)\n",
        "\n",
        "  #print(\"history\", history)\"\"\"\n"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "' #cose aggiunte\\n  hist_df = pd.DataFrame(history.history) \\n\\n  hist_csv_file = \\'history\\' + modelName.replace(\"./drive/MyDrive/\", \"\") + \\'.csv\\'   \\n  with open(hist_csv_file, mode=\\'w\\') as f:\\n    hist_df.to_csv(f)\\n\\n  #print(\"history\", history)'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NQzu1OTiBnFs"
      },
      "source": [
        "\n",
        "def define_CNN_model(embedding_layer, MAX_SEQUENCE_LENGTH):\n",
        "\n",
        "  sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
        "  embedded_sequences = embedding_layer(sequence_input)\n",
        "  x = Conv1D(128, 5, activation='relu')(embedded_sequences)\n",
        "  x = MaxPooling1D(5)(x)\n",
        "  x = Conv1D(128, 5, activation='relu')(x)\n",
        "  x = MaxPooling1D(5)(x)\n",
        "  x = Conv1D(128, 5, activation='relu')(x)\n",
        "  x = MaxPooling1D()(x)  # global max pooling\n",
        "  x = Flatten()(x)\n",
        "  x = Dense(128, activation='relu')(x)\n",
        "  preds = Dense(10, activation='softmax')(x)\n",
        "  \n",
        "  model = keras.Model(sequence_input, preds)\n",
        "\n",
        "  return model\n"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e9nVO9W5FxYX"
      },
      "source": [
        "def define_LSTM_model(embedding_layer, MAX_SEQUENCE_LENGTH,  word_index): \n",
        "  sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
        "  modelLSTM = keras.Sequential()\n",
        "  embedded_sequences = embedding_layer(sequence_input)\n",
        "  modelLSTM.add(embedding_layer)\n",
        "  modelLSTM.add(Bidirectional(LSTM(units=32, dropout=0.3,recurrent_dropout=0.2)))\n",
        "  modelLSTM.add(Dense(32,activation='relu'))\n",
        "  modelLSTM.add(Dropout(0.3))\n",
        "  modelLSTM.add(Dense(10,activation=\"softmax\"))\n",
        "  return modelLSTM\n"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xAUbv2OrIqhi"
      },
      "source": [
        "def training_over_custom_word_embedding(df):\n",
        "  w2v_model = create_custom_word_embedding(df)  \n",
        "  visualize_word2vec_model(w2v_model)\n",
        "  save_pretrained_word_embedding(\"word_embedding.txt\", False, w2v_model)\n",
        "\n",
        "  #get dataset info\n",
        "  max_words_for_sentence = df[\"number_of_words\"].max()\n",
        "  median_words_for_sentence = df[\"number_of_words\"].median()\n",
        "  vocab_len = len(w2v_model.wv.vocab)\n",
        "  print(\"vocab length: \",len(w2v_model.wv.vocab), \"max\", max_words_for_sentence)\n",
        "\n",
        "\n",
        "\n",
        "  MAX_NB_WORDS, MAX_SEQUENCE_LENGTH, VALIDATION_SPLIT, EMBEDDING_DIM = inits_params_word_embedding(len(w2v_model.wv.vocab), max_words_for_sentence, 0.2, 100 )\n",
        "  word_index, x_train, x_test, y_train,y_val, x_val, y_val, y_test = data_preparation(df,MAX_NB_WORDS, MAX_SEQUENCE_LENGTH, VALIDATION_SPLIT, EMBEDDING_DIM)\n",
        "\n",
        " # X_train, X_test, y_train, y_test, x_val, y_val\n",
        "\n",
        "  embeddings_index = load_word_embedding('word_embedding.txt')\n",
        "  embedding_matrix =create_embedding_matrix(embeddings_index, word_index, EMBEDDING_DIM)\n",
        "  if not os.path.isdir(\"./drive/MyDrive/CNN_model_custom_WEskipgram\") or not os.path.isdir(\"./drive/MyDrive/LSTM_model_custom_WEskipgram\") :\n",
        "    print(\"models are training...\")\n",
        "    embedding_layer = inits_params_embedding_layer(embedding_matrix, word_index, MAX_SEQUENCE_LENGTH, EMBEDDING_DIM )\n",
        "    CNN_model_custom_we = define_CNN_model(embedding_layer, MAX_SEQUENCE_LENGTH) \n",
        "    CNN_model_custom_we = run_model(CNN_model_custom_we, x_train, y_train,  x_val, y_val, \"./drive/MyDrive/CNN_model_custom_WEskipgram\")\n",
        "    #getPerformanceMetricsBERT(CNN_model_custom_we, x_test, y_test )\n",
        "\n",
        "    LSTM_model_custom_we = define_LSTM_model(embedding_layer, MAX_SEQUENCE_LENGTH, word_index)\n",
        "    LSTM_model_custom_we.summary()\n",
        "    LSTM_model_custom_we = run_model(LSTM_model_custom_we,  x_train, y_train,  x_val, y_val, \"./drive/MyDrive/LSTM_model_custom_WEskipgram\")\n",
        "\n",
        "    #getPerformanceMetricsBERT(modelLstm, x_test, y_test )\n",
        "  else:\n",
        "    #load trained models\n",
        "    print(\"models already trained, loading...\")\n",
        "    LSTM_model_custom_we = tf.keras.models.load_model('./drive/MyDrive/LSTM_model_custom_WEskipgram',  custom_objects={'get_f1': get_f1})\n",
        "    CNN_model_custom_we = tf.keras.models.load_model('./drive/MyDrive/CNN_model_custom_WEskipgram',  custom_objects={'get_f1': get_f1}) \n",
        "  \n",
        "  return LSTM_model_custom_we, CNN_model_custom_we, x_test, y_test"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d9YQ0JlC_aCu"
      },
      "source": [
        "#### Modeling Using Glove Pre-trained Word Embedding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D8UaNy__Hho3"
      },
      "source": [
        "import requests, zipfile, io\n",
        "def glove_download(): \n",
        "  if  not os.path.isfile('./drive/MyDrive/glove.840B.300d.txt/glove.840B.300d.txt'):\n",
        "    print(\"Downloading... \")\n",
        "    zip_file_url = \"http://nlp.stanford.edu/data/glove.840B.300d.zip\"\n",
        "    r = requests.get(zip_file_url)\n",
        "    z = zipfile.ZipFile(io.BytesIO(r.content))\n",
        "    z.extractall(\"./drive/MyDrive/glove.840B.300d.txt\")\n",
        "  else: \n",
        "    print(\"glove has already been downloaded\")\n",
        "\n",
        "\n"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K4897Bu5BkIC"
      },
      "source": [
        "def training_over_glove_word_embedding(df):\n",
        "  glove_download()\n",
        "  embeddings_index = load_word_embedding('./drive/MyDrive/glove.840B.300d.txt/glove.840B.300d.txt')\n",
        "  #TODO: define MAX_NB_WORDS correctly\n",
        "  max_words_for_sentence = df[\"number_of_words\"].max()\n",
        "  median_words_for_sentence = df[\"number_of_words\"].median()\n",
        " # vocab_len = len(w2v_model.wv.vocab)\n",
        " # print(\"vocab length: \",len(w2v_model.wv.vocab), \"max\", max_words_for_sentence)\n",
        "\n",
        "  MAX_NB_WORDS, MAX_SEQUENCE_LENGTH, VALIDATION_SPLIT, EMBEDDING_DIM = inits_params_word_embedding(len(embeddings_index), max_words_for_sentence, 0.2, 300 )\n",
        "  word_index, x_train, x_test, y_train, y_test, x_val, y_val , y_test = data_preparation(df, MAX_NB_WORDS, MAX_SEQUENCE_LENGTH, VALIDATION_SPLIT, EMBEDDING_DIM)\n",
        "\n",
        "  embedding_matrix =create_embedding_matrix(embeddings_index, word_index, EMBEDDING_DIM)\n",
        "\n",
        "  \n",
        "  if not os.path.isdir(\"./drive/MyDrive/CNN_model_glove_WE\") or not os.path.isdir(\"./drive/MyDrive/LSTM_model_glove_WE\") :\n",
        "    print(\"models are training...\")\n",
        "    embedding_layer = inits_params_embedding_layer(embedding_matrix, word_index, MAX_SEQUENCE_LENGTH, EMBEDDING_DIM )\n",
        "    CNN_model_glove_we = define_CNN_model(embedding_layer, MAX_SEQUENCE_LENGTH) \n",
        "    CNN_model_glove_we = run_model(CNN_model_glove_we, x_train, y_train,  x_val, y_val, \"./drive/MyDrive/CNN_model_glove_WE\")\n",
        "    #getPerformanceMetricsBERT(CNN_model_glove_we, x_test, y_test )\n",
        "\n",
        "    LSTM_model_glove_we = define_LSTM_model(embedding_layer, MAX_SEQUENCE_LENGTH, word_index)\n",
        "    LSTM_model_glove_we.summary()\n",
        "    LSTM_model_glove_we = run_model(LSTM_model_glove_we,  x_train, y_train,  x_val, y_val, \"./drive/MyDrive/LSTM_model_glove_WE\")\n",
        "    #getPerformanceMetricsBERT(LSTM_model_glove_we, x_test, y_test )\n",
        "    \n",
        "  else:\n",
        "    #load trained models\n",
        "    print(\"models already trained, loading...\")\n",
        "    LSTM_model_glove_we = tf.keras.models.load_model('./drive/MyDrive/LSTM_model_glove_WE',  custom_objects={'get_f1': get_f1})\n",
        "    CNN_model_glove_we = tf.keras.models.load_model('./drive/MyDrive/CNN_model_glove_WE',  custom_objects={'get_f1': get_f1}) \n",
        "  \n",
        "  return LSTM_model_glove_we, CNN_model_glove_we, x_test, y_test\n"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IMeyXyCjNlHR"
      },
      "source": [
        "#### Bert Tensorflow "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iC4ksptiStsQ",
        "outputId": "a672dc23-ede5-4814-8697-4975a33020a4"
      },
      "source": [
        "!pip install sentencepiece"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (0.1.95)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NOnU8xYsn-3A"
      },
      "source": [
        "!wget --quiet https://raw.githubusercontent.com/tensorflow/models/master/official/nlp/bert/tokenization.py"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kRqFQiM5Sh2B"
      },
      "source": [
        "from tensorflow.keras.layers import Dense, Input\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "import tensorflow_hub as hub\n",
        "\n",
        "import tokenization"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MQ3gU5osTQCP"
      },
      "source": [
        "### Add tokens to the data make it BERT compatible\n",
        "def bert_encode(test_df, tokenizer, max_len=512):\n",
        "    all_tokens = []\n",
        "    all_masks = []\n",
        "    all_segments = []\n",
        "    \n",
        "    #for text in texts:\n",
        "    for i in tqdm(range(len(test_df))):\n",
        "        text = tokenizer.tokenize(test_df.iloc[i])\n",
        "        \n",
        "        text = text[:max_len-2]\n",
        "        input_sequence = [\"[CLS]\"] + text + [\"[SEP]\"]\n",
        "        pad_len = max_len - len(input_sequence)\n",
        "        \n",
        "        tokens = tokenizer.convert_tokens_to_ids(input_sequence)\n",
        "        tokens += [0] * pad_len\n",
        "        pad_masks = [1] * len(input_sequence) + [0] * pad_len\n",
        "        segment_ids = [0] * max_len\n",
        "        \n",
        "        all_tokens.append(tokens)\n",
        "        all_masks.append(pad_masks)\n",
        "        all_segments.append(segment_ids)\n",
        "    \n",
        "    return np.array(all_tokens), np.array(all_masks), np.array(all_segments)\n",
        "\n",
        "def build_model(bert_layer, max_len=512):\n",
        "    input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n",
        "    input_mask = Input(shape=(max_len,), dtype=tf.int32, name=\"input_mask\")\n",
        "    segment_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"segment_ids\")\n",
        "\n",
        "    _, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])\n",
        "    clf_output = sequence_output[:, 0, :]\n",
        "    print(clf_output.shape)\n",
        "    #model ok #f1 = 82 % \n",
        "    flat_layer = Flatten()(clf_output)\n",
        "    dropout= Dropout(0.2)(flat_layer)\n",
        "\n",
        "    out = Dense(10, activation='softmax')(dropout)\n",
        "    #todo: provare a mettere una cnn al posto del semplice dense\n",
        "    #c'Ã¨ un po di overfitting\n",
        "   \n",
        "    model = Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=out)\n",
        "    model.compile(Adam(lr=2e-6), loss='categorical_crossentropy', metrics=['accuracy', get_f1])\n",
        "\n",
        "    return model\n",
        "\n"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T0hifj37Slq_"
      },
      "source": [
        "def training_with_bert(df, X, y, modelName):\n",
        "  \n",
        "  #split dataset\n",
        "  X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=0.3, random_state = 42)\n",
        "  #X_train, X_test, y_train, y_test, X_val, y_val = split_dataset(df.iloc[:, 0 ],df.iloc[:, 3])\n",
        "  y_train = to_categorical(np.asarray(y_train)) #y \n",
        "  y_test = to_categorical(np.asarray(y_test)) #y\n",
        "  #y_val = to_categorical(np.asarray(y_val))\n",
        "\n",
        "  module_url = \"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1\"\n",
        "  #module_url = \"https://tfhub.dev/tensorflow/bert_en_uncased_L-24_H-1024_A-16/1\"\n",
        "\n",
        "  bert_layer = hub.KerasLayer(module_url, trainable=True)\n",
        "\n",
        "  vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
        "  do_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n",
        "\n",
        "  tokenizer = tokenization.FullTokenizer(vocab_file, do_lower_case)\n",
        "  train_input = bert_encode(X_train, tokenizer, max_len=128)\n",
        "  test_input = bert_encode(X_test, tokenizer, max_len=128)\n",
        "  #val_input = bert_encode(X_val, tokenizer,  max_len=128)\n",
        "\n",
        "  if not os.path.isdir(modelName) :\n",
        "    print(\"models are training...\")\n",
        "\n",
        "    bert = build_model(bert_layer, max_len=128)\n",
        "    bert.summary()\n",
        "    es_callback = EarlyStopping(monitor='val_loss', patience=3)\n",
        "    history = bert.fit(\n",
        "          train_input, y_train,\n",
        "          validation_split=0.2, \n",
        "          epochs=25,\n",
        "          batch_size=16,\n",
        "          callbacks=[es_callback]\n",
        "    )\n",
        "    bert.save(modelName)\n",
        "    print(\"history\", history)\n",
        "    hist_df = pd.DataFrame(history.history) \n",
        "\n",
        "    hist_csv_file = './drive/MyDrive/history/history' + modelName.replace(\"./drive/MyDrive/\", \"\") + '.csv'   \n",
        "    with open(hist_csv_file, mode='w') as f:\n",
        "      hist_df.to_csv(f)\n",
        "\n",
        "    plot_accuracy_loss(history)\n",
        "    getPerformanceMetricsBERT(bert, test_input, y_test)\n",
        "  else: \n",
        "    #load trained models\n",
        "    print(\"models already trained, loading...\")\n",
        "    bert = tf.keras.models.load_model(modelName,  custom_objects={'get_f1': get_f1})\n",
        "   # bert_no_preprocess = tf.keras.models.load_model('./drive/MyDrive/BERT_model_preProcessing',  custom_objects={'get_f1': get_f1}) \n",
        "  \n",
        "  return bert, test_input, y_test\n"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TbbOB0LFPU4D",
        "outputId": "73c73e1a-8992-47b8-d02e-03b1e939195f"
      },
      "source": [
        "df = get_data()\n",
        "df , categories= pre_processing(df)\n",
        "\n",
        "categoryDistribution = df.groupby([\"label\"])[\"content\"].count()\n",
        "plt.pyplot.bar(categories, categoryDistribution, width = 0.7, bottom=None, align='center', data=None)\n",
        "#visualize mean of sentence lenght for each category\n",
        "df.insert(1, 'number_of_words', df.cleaned_text.apply(lambda x: len(x)))\n",
        "median_words_for_sentence = df.groupby(\"label\")[\"number_of_words\"].median()\n",
        "plt.pyplot.bar(categories, median_words_for_sentence, width = 0.8, bottom=None, align='center', data=None)\n",
        "\n",
        "#df.groupby('label').apply(lambda x: word_cloud(x))\n",
        "\n",
        "#cast cleaned text into string\n",
        "df['cleaned_text'] = df['cleaned_text'].apply(lambda x: ', '.join(map(str, x)))\n",
        "df['cleaned_text'] = df['cleaned_text'].apply(lambda x: x.replace(\",\", \"\"))\n",
        "#X_train, X_test, y_train, y_test, X_val, y_val = split_dataset(df.iloc[:, 0 ],df.iloc[:, 4])  #cleaned text and string label\n",
        "compute_baseline(df)\n",
        "\"\"\"\n",
        "LSTM_model_custom_we, CNN_model_custom_we, x_test, y_test = training_over_custom_word_embedding(df)\n",
        "\n",
        "getPerformanceMetricsBERT(LSTM_model_custom_we, x_test, y_test, \"LSTM_model_custom_we_skipgram\" )\n",
        "getPerformanceMetricsBERT(CNN_model_custom_we, x_test, y_test, \"CNN_model_custom_we_skipgram\" )\n",
        "\n",
        "\n",
        "LSTM_model_glove_we, CNN_model_glove_we, x_test, y_test = training_over_glove_word_embedding(df)\n",
        "getPerformanceMetricsBERT(CNN_model_glove_we, x_test, y_test, \"CNN_model_glove_we\" )\n",
        "getPerformanceMetricsBERT(LSTM_model_glove_we, x_test, y_test , \"LSTM_model_glove_we\")\n",
        "\"\"\"\n",
        "cleaned_text = df[\"cleaned_text\"]\n",
        "raw_text = df[\"content\"]\n",
        "bert_no_preprocess, x_test, y_test = training_with_bert(df, raw_text, df.iloc[:, 4], \"./drive/MyDrive/BERT_model_no_preProcessing1\")\n",
        "getPerformanceMetricsBERT(bert_no_preprocess, x_test, y_test, \"bert_no_preprocess\" )\n",
        "\n",
        "bert_preprocess, x_test, y_test = training_with_bert(df, cleaned_text, df.iloc[:, 4], \"./drive/MyDrive/BERT_model_preProcessing1\")\n",
        "getPerformanceMetricsBERT(bert_preprocess, x_test, y_test, \"bert_preprocess\" )\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dataset has already been downloaded. \n",
            "Dataset files have already been joined. \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:15: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  from ipykernel import kernelapp as app\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:19: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "accuracy 0.7420596727622714\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      Report       0.81      0.59      0.68        73\n",
            "        Note       0.86      0.97      0.91       184\n",
            "       Email       0.78      0.83      0.81       132\n",
            "        ADVE       0.74      0.75      0.74       169\n",
            "      Letter       0.68      0.85      0.75       183\n",
            "  Scientific       0.58      0.84      0.68        49\n",
            "      Resume       0.73      0.49      0.59        65\n",
            "        News       0.78      0.19      0.31        93\n",
            "        Memo       0.90      1.00      0.95        27\n",
            "        Form       0.60      0.62      0.61        64\n",
            "\n",
            "    accuracy                           0.74      1039\n",
            "   macro avg       0.74      0.71      0.70      1039\n",
            "weighted avg       0.75      0.74      0.72      1039\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|ââââââââââ| 2423/2423 [00:10<00:00, 228.39it/s]\n",
            "100%|ââââââââââ| 1039/1039 [00:04<00:00, 238.10it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "models are training...\n",
            "(None, 768)\n",
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_word_ids (InputLayer)     [(None, 128)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_mask (InputLayer)         [(None, 128)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "segment_ids (InputLayer)        [(None, 128)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "keras_layer (KerasLayer)        [(None, 768), (None, 109482241   input_word_ids[0][0]             \n",
            "                                                                 input_mask[0][0]                 \n",
            "                                                                 segment_ids[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "tf.__operators__.getitem (Slici (None, 768)          0           keras_layer[0][1]                \n",
            "__________________________________________________________________________________________________\n",
            "flatten (Flatten)               (None, 768)          0           tf.__operators__.getitem[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "dropout (Dropout)               (None, 768)          0           flatten[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 10)           7690        dropout[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 109,489,931\n",
            "Trainable params: 109,489,930\n",
            "Non-trainable params: 1\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/25\n",
            " 35/122 [=======>......................] - ETA: 31:13 - loss: 2.4087 - accuracy: 0.1533 - get_f1: 0.0000e+00"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mzlv6duiSUAD"
      },
      "source": [
        "#to do unica funzione per ogni possibile modello, se giÃ  fatto il training e salvato, allora non fare piÃ¹ il training ma salvare \n",
        "#per ogni modello dare le metriche, dopo di chÃ¨ magari provare a fare la classificazione di un singolo sample.\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "URfs8KEhoj8G"
      },
      "source": [
        "#single plot for f1 values, and loss values got from each model \n",
        "def compare_curves(): \n",
        "  plt.pyplot.subplot(211)\n",
        "  plt.pyplot.title('model f1_score')\n",
        "  plt.pyplot.ylabel('f1_score')\n",
        "  plt.pyplot.xlabel('epoch')\n",
        "  mod = []\n",
        "  \n",
        "  for dirpath, dirs, filenames in os.walk(\"./drive/MyDrive/history/\"):\n",
        "    for filename in filenames:\n",
        "      mod.append(filename)\n",
        "      hist_df = pd.read_csv(dirpath +\"/\"+filename, sep=',', delimiter=None )\n",
        "      plt.pyplot.plot(hist_df['val_get_f1'])\n",
        "  \n",
        "  plt.pyplot.subplot(212)\n",
        "  plt.pyplot.title('model loss value')\n",
        "  plt.pyplot.ylabel('loss')\n",
        "  plt.pyplot.xlabel('epoch')\n",
        "\n",
        "  for dirpath, dirs, filenames in os.walk(\"./drive/MyDrive/history/\"):\n",
        "    for filename in filenames:\n",
        "      mod.append(filename)\n",
        "      hist_df = pd.read_csv(dirpath +\"/\"+filename, sep=',', delimiter=None )\n",
        "      plt.pyplot.plot(hist_df['val_loss'])\n",
        "  \n",
        "\n",
        "\n",
        "  plt.pyplot.legend(mod, loc='upper left')\n",
        "  plt.pyplot.show()\n",
        "\n",
        "  \n",
        "compare_curves()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BJoBSmXIzLsT"
      },
      "source": [
        "hist_df = pd.read_csv(\"./drive/MyDrive/history/historyCNN_model_custom_WE1.csv\", sep=',', delimiter=None )  \n",
        "hist_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8-UVJ5rZz-y1"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}