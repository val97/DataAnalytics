# -*- coding: utf-8 -*-
"""textRecognition.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/19Jyr0fPFHdaf8W4ghNxG5cqge4MOy3rB
"""

# Commented out IPython magic to ensure Python compatibility.
import os
import sys
import csv
import pandas as pd
import matplotlib as plt
# %matplotlib inline
import numpy as np
from sklearn.preprocessing import LabelEncoder
import seaborn as sns

""" ## Data Acquisition



"""

def downloadDataset():
  if  not os.path.isfile('./QS-OCR-small.tar.gz'):
    print("Downloading... ")
    ! wget https://github.com/QuickSign/ocrized-text-dataset/releases/download/v1.0/QS-OCR-small.tar.gz -P ./
    !mkdir dataset
    !tar -xf QS-OCR-small.tar.gz -C ./dataset
  else: 
    print("Dataset has already been downloaded. ")

def createSingleDataset():
  # join all the file content into a single csv file
  if not os.path.isfile('./dataset.csv'):
    print("joining..")
    path = "./dataset"
    file_dataset = open('dataset.csv', mode='w')
    file_writer = csv.writer(file_dataset, delimiter='|', quotechar='"', quoting=csv.QUOTE_MINIMAL)
    #walk trough the directory to get all the files' content
    for dirpath, dirs, filenames in os.walk(path):
      for filename in filenames:
        f = open(dirpath +"/"+filename, "r")
        getfilebody = f.read()
        file_writer.writerow([filename, getfilebody,  dirpath.replace("./dataset/", "") ] )
        f.close()
  else:
      print("Dataset files have already been joined. ")

downloadDataset()
createSingleDataset()

import tensorflow as tf
tf.test.gpu_device_name()

"""##Pre-Processing
Data cleaning:
  >* filling in missing values,
  >*  smoothing noisy data,
  >* identifying or removing outliers,
  >* resolving inconsistencies,
  >* duplicate samples
  >* NLP pipeline = data normalizzation for text
  >>* Tokenization
  >>* punctuation removal
  >>* stop words removal
  >>* noise removal: Remove HTML tags, remove extra whitespaces, 
  >>* normalization: typo correction,  converting all characters to lowercase, number, date, symbol textualization, Expand contractions, remove_accented_chars
  >>* lemmatization o steaming 

"""

df = pd.read_csv("dataset.csv", sep='|', delimiter=None, names = ["filename","content", "label"] )
df.dropna

#label encoding
categories = pd.unique(df["label"])

labelencoder = LabelEncoder()

df['y'] = labelencoder.fit_transform(df['label'])
df

!pip install langid
import spacy 
import en_core_web_sm
import langid
import re as re

def remove_tags(string):
    result = re.sub('<.*?>',' ',string)
    return result

def clean_text(text):
  for t in nlp.tokenizer(text):
    if ((not t.is_stop) and ( not t.is_punct) and (not t.is_currency) and (not t.is_digit) and (not t.is_space) and (t.is_alpha) ):
      return t.lemma_.lower().join(t.pos_)

def execute_nlp(df):
  nlp = spacy.load('en')  
  #speech tag is slow
  #df['tagged_text'] = df['content_without_tags'].apply(lambda x: [t.pos_ for t in nlp(x) if ((not t.is_stop) and ( not t.is_punct) and (not t.is_currency) and (not t.is_digit) and (not t.is_space) and (t.is_alpha) ) ])
  df.insert(1, 'cleaned_text', df['content_without_tags'].apply(lambda x: [t.lemma_.lower() for t in nlp.tokenizer(x) if ((not t.is_stop) and ( not t.is_punct) and (not t.is_currency) and (not t.is_digit) and (not t.is_space) and (t.is_alpha) ) ]))
  return df

df['content']=df['content'].apply(str)
df.insert(1, 'content_without_tags', df["content"].apply(lambda cw : remove_tags(cw)))  

df = execute_nlp(df)

df = df.drop("content_without_tags", axis = 1)
df = df.drop("filename", axis = 1)

nlp = spacy.load('en')  

prova = nlp("ciao come stai? ")
prova
for token in prova: 
  print(token.text, ': ', token.pos_)
 #df['cleaned_text'].apply(lambda x: [t.pos for t in x  ])

"""## Visualization and Dataset Analysis"""

df

categoryDistribution = df.groupby(["label"])["content"].count()
plt.pyplot.bar(categories, categoryDistribution, width = 0.8, bottom=None, align='center', data=None)

#visualize mean of sentence lenght for each category
df.insert(1, 'number_of_words', df.cleaned_text.apply(lambda x: len(x)))
median_words_for_sentence = df.groupby("label")["number_of_words"].median()
plt.pyplot.bar(categories, median_words_for_sentence, width = 0.8, bottom=None, align='center', data=None)

"""## Baseline Model """

## for bag-of-words
from sklearn import feature_extraction, model_selection, manifold, preprocessing, feature_selection
from sklearn.naive_bayes import MultinomialNB
from sklearn.linear_model import SGDClassifier
from sklearn.pipeline import Pipeline

df['cleaned_text'] = df['cleaned_text'].apply(lambda x: ', '.join(map(str, x)))
df['cleaned_text'] = df['cleaned_text'].apply(lambda x: x.replace(",", ""))

## split dataset
def split_dataset(X, y):
  #X = df.iloc[:, 0 ]
 # y = df.iloc[:, 4 ]
  X_train, X_test, y_train, y_test= model_selection.train_test_split(X, y, test_size=0.3, random_state = 42)
  X_train, X_val, y_train, y_val = model_selection.train_test_split(X_train, y_train, test_size=0.2, random_state = 42)

  #y_test = y_test.values.reshape((1045,1))
  return X_train, X_test, y_train, y_test, X_val, y_val

X_train, X_test, y_train, y_test, X_val, y_val = split_dataset(df.iloc[:, 0 ],df.iloc[:, 4 ])

print(X_train.shape)
print(y_train.shape)
print(X_test.shape)
print(y_test.shape)
print(X_val.shape)
print(y_val.shape)

#categories = ['ADVE','Email','Form','Letter','Memo','News','Note','Report','Resume','Scientific']

def multinomialNBmodel():
  nb = Pipeline([('vectorizer', feature_extraction.text.CountVectorizer()),
                ('tfidf', feature_extraction.text.TfidfTransformer()),
               ('clf', MultinomialNB()),])
  #nb.fit(X_train, y_train)
  return nb
def SVMmodel():
  svm = Pipeline([('vectorizer', feature_extraction.text.CountVectorizer()),
                ('tfidf', feature_extraction.text.TfidfTransformer()),
               ('clf',  SGDClassifier(penalty='l2', alpha=1e-3, random_state=42)),])
  #nb.fit(X_train, y_train)
  return svm

def getPerformanceMetrics(model):
  from sklearn.metrics import classification_report, accuracy_score,confusion_matrix,roc_curve,auc,precision_recall_curve
  y_pred = model.predict(X_test)
  #predicted_prob = model.predict_proba(X_test)
  print('accuracy %s' % accuracy_score(y_pred, y_test))
  print(classification_report(y_test, y_pred, target_names=categories))
   ## Plot confusion matrix
  cm = confusion_matrix(y_test, y_pred)
  fig, ax = plt.pyplot.subplots()
  sns.heatmap(cm, annot=True, fmt='d', ax=ax, cmap=plt.cm.Blues, 
              cbar=False)
  ax.set(xlabel="Pred", ylabel="True", xticklabels=categories, 
        yticklabels=categories, title="Confusion matrix")
  plt.pyplot.yticks(rotation=0)
  fig, ax = plt.pyplot.subplots(nrows=1, ncols=2)
  ## Plot roc
  """y_test_array = pd.get_dummies(y_test, drop_first=False).values
  for i in range(len(categories)):
      fpr, tpr, thresholds = roc_curve(y_test_array[:,i],  
                            predicted_prob[:,i])
      ax[0].plot(fpr, tpr, lw=3, 
                label='{0} (area={1:0.2f})'.format(categories[i], 
                                auc(fpr, tpr))
                )
  ax[0].plot([0,1], [0,1], color='navy', lw=3, linestyle='--')
  ax[0].set(xlim=[-0.05,1.0], ylim=[0.0,1.05], 
            xlabel='False Positive Rate', 
            ylabel="True Positive Rate (Recall)", 
            title="Receiver operating characteristic")
  ax[0].legend(loc="lower right")
  ax[0].grid(True)
      
  ## Plot precision-recall curve
   for i in range(len(categories)):
      precision, recall, thresholds = precision_recall_curve(
                  y_test_array[:,i], predicted_prob[:,i])
      ax[1].plot(recall, precision, lw=3, 
                label='{0} (area={1:0.2f})'.format(categories[i], 
                                    auc(recall, precision))
                )
  ax[1].set(xlim=[0.0,1.05], ylim=[0.0,1.05], xlabel='Recall', 
            ylabel="Precision", title="Precision-Recall curve")
  ax[1].legend(loc="best")
  ax[1].grid(True)
  plt.pyplot.show()"""
 

#NOTE: with MultinomialNB and tfidf there are label that are not predicted

#model = multinomialNBmodel()
model = SVMmodel()
model.fit(X_train, y_train)

getPerformanceMetrics(model)

#TO DO: tuning of parameters with searchgrid tfidf(True, False), ngrams

"""## Modeling - Word Embeddings

####Create Customized Word Embedding
"""

from keras.layers import Embedding, Conv1D, MaxPooling1D, Flatten, Dense,LSTM, GRU,Bidirectional,Dropout
from tensorflow.keras import Input
from keras.initializers import Constant
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.utils import to_categorical
from tensorflow.keras import layers
from tensorflow import keras
import keras.backend as K
from keras.layers import BatchNormalization
import tensorflow as tf
from keras.optimizers import RMSprop
from keras.callbacks import ModelCheckpoint
from tensorflow.keras.callbacks import EarlyStopping

from gensim.models import Word2Vec, KeyedVectors
from collections import defaultdict

def get_most_frequent_word(number_of_word_to_get):
  sentences = [row.split() for row in df['cleaned_text']] #tokenize sentence

  #create dictionary word_freq(word, number_of_occurence)
  word_freq = defaultdict(int)
  for sent in sentences:
      for i in sent:
        # print(sent)
          #print(i)
          word_freq[i] += 1
          
  print("top 10 most frequent words:", sorted(word_freq, key=word_freq.get, reverse=True)[:number_of_word_to_get])
  #word_freq

get_most_frequent_word(13)

def create_custom_word_embedding():
    sentences = [row.split() for row in df['cleaned_text']] #tokenize sentence
    w2v_model = Word2Vec(min_count=100, #min_count: minimum amount of time a word appear in word2vec training corpus  
                        window=5,  # context window +- center word
                        size=100,  # dimensions of word embeddings, also refer to size of hidden layer
                        workers=4)
                        
    w2v_model.build_vocab(sentences)
    w2v_model.train(sentences, total_examples=w2v_model.corpus_count, epochs=w2v_model.iter)
    w2v_model.init_sims(replace=True)
    print("vocab length",len(w2v_model.wv.vocab))
    return  w2v_model

w2v_model = create_custom_word_embedding()

#testing word embedding
#Find the most similar words for "smoke"
w2v_model.wv.most_similar(positive=['study'])
w2v_model.wv.most_similar(positive=['cigarette'])

from sklearn.manifold import TSNE
def visualize_word2vec_model(model):
    "Create TSNE model and plot it"
    labels = []
    tokens = []

    for word in model.wv.vocab:
        tokens.append(model[word])
        labels.append(word)
    
    tsne_model = TSNE(perplexity=40, n_components=2, init='pca', n_iter=2500, random_state=23)
    new_values = tsne_model.fit_transform(tokens)

    x = []
    y = []
    for value in new_values:
        x.append(value[0])
        y.append(value[1])
        
    plt.pyplot.figure(figsize=(18, 18)) 
    for i in range(len(x)):
        plt.pyplot.scatter(x[i],y[i])
        plt.pyplot.annotate(labels[i],
                     xy=(x[i], y[i]),
                     xytext=(5, 2),
                     textcoords='offset points',
                     ha='right',
                     va='bottom')
    plt.pyplot.show()

def save_pretrained_word_embedding(filename, isBin):
  w2v_model.wv.save_word2vec_format(filename, binary = isBin)

"""#### Modeling Using Custom Word Embedding"""

def inits_params_word_embedding(vocab_len, max_sequence_length, val_split, embedding_dim ):
  MAX_NB_WORDS = vocab_len
  MAX_SEQUENCE_LENGTH = max_sequence_length # 800
  VALIDATION_SPLIT = val_split  #0.2
  EMBEDDING_DIM = embedding_dim #300 #has to be between 100 and 300
  
  return MAX_NB_WORDS, MAX_SEQUENCE_LENGTH, VALIDATION_SPLIT, EMBEDDING_DIM

def data_preparation(MAX_NB_WORDS, MAX_SEQUENCE_LENGTH, VALIDATION_SPLIT, EMBEDDING_DIM):
  #convert data into vector to fit word embedding:
    # it means vectorize a text corpus, by turning each text into either a sequence of integers 
  tokenizer = Tokenizer(nb_words=MAX_NB_WORDS)
  tokenizer.fit_on_texts(df["cleaned_text"])   #In the case where texts contains lists, we assume each entry of the lists to be a token.
  sequences = tokenizer.texts_to_sequences(df["cleaned_text"])  #turn each list into a "sequence": is a list of integer word indices. 
  word_index = tokenizer.word_index
  print('Found %s unique tokens.' % len(word_index))

  data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH) #X

  labels = to_categorical(np.asarray(df["y"])) #y
  print('Shape of data tensor:', data.shape)
  print('Shape of label tensor:', labels.shape)

  # split the data into a training set and a validation set
  
  #####TODO: can't I do with test_set_split?
  ######TODO: split data preparation function into data_preparation and train_and_test_split

  X_train, X_test, y_train, y_test, x_val, y_val = split_dataset(data, labels)
    
  return word_index, X_train, X_test, y_train, y_test, x_val, y_val

from tqdm import tqdm
def load_word_embedding(filename):
  #extracting vocabs from 'word_embedding.txt'
  embeddings_index = {}
  f = open(os.path.join("", filename),encoding='utf-8' )
  for line in tqdm(f):
      values = line.rstrip().rsplit(' ')
      word = values[0]
      coefs = np.asarray(values[1:], dtype='float32')
      embeddings_index[word] = coefs
  f.close()
  print('Found %s word vectors.' % len(embeddings_index))

  return embeddings_index

#create embedding matrix that join pretrained word embedding with the dataset's vocabulary
def create_embedding_matrix(embeddings_index, word_index):
  embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))
  for word, i in word_index.items():
      embedding_vector = embeddings_index.get(word)
      if embedding_vector is not None:
          # words not found in embedding index will be all-zeros.
          embedding_matrix[i] = embedding_vector
  return embedding_matrix

def inits_params_embedding_layer(embedding_matrix, word_index, MAX_SEQUENCE_LENGTH, EMBEDDING_DIM):
  print("embedding_layer")

  embedding_layer = Embedding(len(word_index) + 1,
                            EMBEDDING_DIM,
                            weights=[embedding_matrix],
                            input_length=MAX_SEQUENCE_LENGTH,
                            trainable=False)
  return embedding_layer

def get_f1(y_true, y_pred): #taken from old keras source code
    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))
    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))
    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))
    precision = true_positives / (predicted_positives + K.epsilon())
    recall = true_positives / (possible_positives + K.epsilon())
    f1_val = 2*(precision*recall)/(precision+recall+K.epsilon())
    
    return f1_val

def run_model(model):
  model.compile(loss='categorical_crossentropy',
                optimizer='rmsprop',
                metrics=[get_f1, "accuracy"])
  es_callback = EarlyStopping(monitor='val_loss', patience=3)
  #history = model.fit(x_train, y_train, batch_size=128, epochs=30, validation_data=(x_val, y_val), callbacks=[es_callback], shuffle=False)



  # happy learning!
  model.fit(x_train, y_train, validation_data=(x_val, y_val),
            epochs=20, batch_size=128,  callbacks=[es_callback], shuffle=False)

def define_CNN_model(embedding_layer, MAX_SEQUENCE_LENGTH):

  sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')
  embedded_sequences = embedding_layer(sequence_input)
  x = Conv1D(128, 5, activation='relu')(embedded_sequences)
  x = MaxPooling1D(5)(x)
  x = Conv1D(128, 5, activation='relu')(x)
  x = MaxPooling1D(5)(x)
  x = Conv1D(128, 5, activation='relu')(x)
  x = MaxPooling1D(27)(x)  # global max pooling
  x = Flatten()(x)
  x = Dense(128, activation='relu')(x)
  preds = Dense(10, activation='softmax')(x)
  
  model = keras.Model(sequence_input, preds)

  return model

def define_LSTM_model(embedding_layer, MAX_SEQUENCE_LENGTH,  word_index): 
  sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')
  modelLSTM = keras.Sequential()
  embedded_sequences = embedding_layer(sequence_input)
  modelLSTM.add(embedding_layer)
  modelLSTM.add(Bidirectional(LSTM(units=32, dropout=0.3,recurrent_dropout=0.2)))
  modelLSTM.add(Dense(32,activation='relu'))
  modelLSTM.add(Dropout(0.3))
  modelLSTM.add(Dense(10,activation="softmax"))
  """  from keras.layers import BatchNormalization
  import tensorflow as tf
  model = tf.keras.Sequential()
  model.add(Embedding(len(word_index) + 1, EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH, weights=[embedding_matrix],trainable=False))
  model.add(Bidirectional(LSTM(32, return_sequences= True)))
  model.add(Dense(32,activation='relu'))
  model.add(Dropout(0.3))
  model.add(Dense(10 ,activation='softmax'))"""
  return modelLSTM

w2v_model = create_custom_word_embedding()  
visualize_word2vec_model(w2v_model)
save_pretrained_word_embedding("word_embedding.txt", False)

#get dataset info
max_words_for_sentence = df["number_of_words"].max()
median_words_for_sentence = df["number_of_words"].median()
vocab_len = len(w2v_model.wv.vocab)
print("vocab length: ",len(w2v_model.wv.vocab), "max", max_words_for_sentence)



MAX_NB_WORDS, MAX_SEQUENCE_LENGTH, VALIDATION_SPLIT, EMBEDDING_DIM = inits_params_word_embedding(len(w2v_model.wv.vocab),max_words_for_sentence, 0.2, 100 )
word_index, x_train, x_test, y_train,y_val, x_val, y_val = data_preparation(MAX_NB_WORDS, MAX_SEQUENCE_LENGTH, VALIDATION_SPLIT, EMBEDDING_DIM)

X_train, X_test, y_train, y_test, x_val, y_val

embeddings_index = load_word_embedding('word_embedding.txt')
embedding_matrix =create_embedding_matrix(embeddings_index, word_index)

embedding_layer = inits_params_embedding_layer(embedding_matrix, word_index, MAX_SEQUENCE_LENGTH, EMBEDDING_DIM )
CNNmodel = define_CNN_model(embedding_layer, MAX_SEQUENCE_LENGTH) 
run_model(CNNmodel)

#LSTMmodel = define_LSTM_model(embedding_layer, MAX_SEQUENCE_LENGTH, word_index) 
#LSTMmodel.summary()
#run_model(LSTMmodel)

"""#### Modeling Using Glove Pre-trained Word Embedding"""

import requests, zipfile, io
def glove_download(): 
  if  not os.path.isfile('./glove.840B.300d.txt'):
    print("Downloading... ")
    zip_file_url = "http://nlp.stanford.edu/data/glove.840B.300d.zip"
    r = requests.get(zip_file_url)
    z = zipfile.ZipFile(io.BytesIO(r.content))
    z.extractall()
  else: 
    print("glove has already been downloaded")

glove_download()
embeddings_index = load_word_embedding('glove.840B.300d.txt')

len(embeddings_index)

#TODO: define MAX_NB_WORDS corretly
MAX_NB_WORDS, MAX_SEQUENCE_LENGTH, VALIDATION_SPLIT, EMBEDDING_DIM = inits_params_word_embedding(len(word_index)+1 ,max_words_for_sentence, 0.2, 300 )
word_index, x_train, x_test, y_train, y_test, x_val, y_val = data_preparation(MAX_NB_WORDS, MAX_SEQUENCE_LENGTH, VALIDATION_SPLIT, EMBEDDING_DIM)

embedding_matrix =create_embedding_matrix(embeddings_index, word_index)

modelLstm = define_LSTM_model(embedding_layer, MAX_SEQUENCE_LENGTH, word_index)
modelLstm.summary()

run_model(modelLstm)

embedding_layer = inits_params_embedding_layer(embedding_matrix, word_index, MAX_SEQUENCE_LENGTH, EMBEDDING_DIM )

modelCnn = define_CNN_model(embedding_layer, MAX_SEQUENCE_LENGTH)
modelCnn.summary()

run_model(modelCnn)



"""####Fine-tuning of BERT Neural Network
Note: MAX_SEQUENCE_LENGTH = 512 for BERT.


"""

!pip install keras-bert
!pip install keras-rectified-adam

!wget -q https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-12_H-768_A-12.zip
!unzip -o uncased_L-12_H-768_A-12.zip

from keras_radam import RAdam
from keras_bert import load_trained_model_from_checkpoint

SEQ_LEN = 128 #MAX_SEQ_LENGTH = SEQ_LEN is a number of lengths of the sequence after tokenizing. It is set to 128. BERT has worked on at max 512 sequence length.

BATCH_SIZE = 50 
EPOCH= 7 
LR = 1e-4   #LR is the learning rate.

pretrained_path = 'uncased_L-12_H-768_A-12'
config_path = os.path.join(pretrained_path, 'bert_config.json')
checkpoint_path = os.path.join(pretrained_path, 'bert_model.ckpt')
vocab_path = os.path.join(pretrained_path, 'vocab.txt')

model = load_trained_model_from_checkpoint(
      config_path,
      checkpoint_path,
      training=True,
      trainable=True,
      seq_len=SEQ_LEN,
  )

model.summary()

import codecs
from keras_bert import Tokenizer
token_dict = {}
with codecs.open(vocab_path, 'r', 'utf8') as reader:
    for line in reader:
        token = line.strip()
        token_dict[token] = len(token_dict)
        
# print(token_dict)

tokenizer = Tokenizer(token_dict)

